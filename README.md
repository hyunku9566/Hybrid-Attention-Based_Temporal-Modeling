# ğŸ  Lightweight ADL Recognition with TCN-BiGRU-Attention

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A lightweight yet highly accurate deep learning model for Activities of Daily Living (ADL) recognition in smart home environments.

## ğŸ† Key Results

### Best Model: Local Attention Hybrid ğŸ¥‡
- **97.52% Test Accuracy** (State-of-the-art)
- **0.9717 Macro F1-Score**
- **3.32M Parameters** (Real-time capable)
- **+2.12%p improvement** over baseline

### Baseline Model: TCN-BiGRU-Attention ğŸ¥ˆ
- **95.40% Test Accuracy** (Production-ready)
- **0.9468 Macro F1-Score**
- **2.26M Parameters** (Most efficient)
- **Excellent parameter/performance ratio**

## ğŸ¯ Highlights

We evaluated **5 different architectures** and achieved:
- âœ… **97.52% accuracy** with Local Attention Hybrid (best)
- âœ… **95.40% accuracy** with Baseline (most efficient)
- âœ… **>95% F1 on ALL classes** with Local Hybrid
- âœ… **Interpretable** through attention weight visualization
- âœ… **Real-time inference** on edge devices

## ğŸ“Š Model Comparison

| Rank | Model | Test Acc | F1 Score | Parameters | Status |
|------|-------|----------|----------|------------|--------|
| ğŸ¥‡ | **Local Attention Hybrid** | **97.52%** | **0.9717** | 3.32M | â­ Best Overall |
| ğŸ¥ˆ | **Baseline (TCN-BiGRU)** | **95.40%** | **0.9468** | 2.26M | âš¡ Most Efficient |
| ğŸ¥‰ | Deep TCN | 93.42% | 0.9174 | 2.43M | Good |
| 4 | Conformer | 92.08% | 0.9009 | 6.15M | Overparameterized |
| 5 | Transformer | 83.66% | 0.8042 | 3.22M | Not Recommended |

**Full comparison:** See [docs/COMPREHENSIVE_MODEL_COMPARISON.md](docs/COMPREHENSIVE_MODEL_COMPARISON.md)

## ğŸ—ï¸ Architectures

### 1. Local Attention Hybrid ğŸ¥‡ (Best Performance)

```
Input (T=100, F=114)
       â†“
[Feature Projection] Linear(114 â†’ 256)
       â†“
[Temporal Encoding] TCN (5 blocks, dilation=1,2,4,8,16)
       â†“
[Local Self-Attention] Window=25, 2 heads
       â†“
[Sequential Modeling] BiGRU (hidden=256, bidirectional)
       â†“
[Global Attention] Additive Attention
       â†“
[Classification] FC â†’ ReLU â†’ Dropout â†’ FC(5 classes)
       â†“
Output: [cooking, hand_washing, sleeping, medicine, eating]
```

**Key Components:**
- **5-block TCN** with exponential dilation [1,2,4,8,16]
- **Local Self-Attention** (window=25) for efficient local pattern capture
- **BiGRU** for bidirectional sequential modeling
- **Global Attention** for final temporal aggregation
- **Residual connections** + LayerNorm for stable training

**Performance:** 97.52% accuracy, 0.9717 F1

---

### 2. Baseline (TCN-BiGRU-Attention) ğŸ¥ˆ (Most Efficient)

```
Input (T=100, F=114)
       â†“
[Feature Projection] Linear(114 â†’ 256)
       â†“
[Temporal Encoding] TCN (3 blocks, dilation=1,2,4)
       â†“
[Sequential Modeling] BiGRU (hidden=256, bidirectional)
       â†“
[Temporal Attention] Additive Attention (Bahdanau-style)
       â†“
[Classification] FC â†’ ReLU â†’ Dropout â†’ FC(5 classes)
       â†“
Output: [cooking, hand_washing, sleeping, medicine, eating]
```

**Key Components:**
- **3-block TCN** with dilations [1,2,4] (receptive field: 13)
- **BiGRU** (25% fewer params than BiLSTM)
- **Additive Attention** (interpretable, single-focus)

**Performance:** 95.40% accuracy, 0.9468 F1, only 2.26M params

## ğŸ“‚ Project Structure

```
baseline-adl-recognition/
â”œâ”€â”€ data/                       # Data preprocessing
â”‚   â”œâ”€â”€ preprocess.py          # Main preprocessing pipeline
â”‚   â”œâ”€â”€ build_features.py      # Feature extraction from sensors
â”‚   â””â”€â”€ README.md              # Data format documentation
â”œâ”€â”€ models/                     # Model architectures
â”‚   â”œâ”€â”€ baseline_model.py      # TCN-BiGRU-Attention (baseline)
â”‚   â”œâ”€â”€ local_attention_hybrid.py # Local Hybrid (best)
â”‚   â”œâ”€â”€ deep_tcn_model.py      # Deep TCN with SE attention
â”‚   â”œâ”€â”€ conformer_model.py     # Conformer architecture
â”‚   â”œâ”€â”€ transformer_model.py   # Transformer encoder
â”‚   â”œâ”€â”€ components.py          # Shared components (TCN, Attention, FocalLoss)
â”‚   â””â”€â”€ README.md              # Architecture details
â”œâ”€â”€ train/                      # Training scripts
â”‚   â”œâ”€â”€ train.py               # Baseline training script
â”‚   â”œâ”€â”€ train_transformer.py   # Transformer training script
â”‚   â”œâ”€â”€ config.py              # Hyperparameters and settings
â”‚   â””â”€â”€ utils.py               # Training utilities
â”œâ”€â”€ evaluate/                   # Evaluation scripts
â”‚   â”œâ”€â”€ evaluate.py            # Model evaluation
â”‚   â””â”€â”€ visualize.py           # Attention visualization
â”œâ”€â”€ checkpoints/                # Model checkpoints
â”‚   â”œâ”€â”€ best_baseline.pt       # Baseline checkpoint (95.40%)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ checkpoints_local_hybrid/   # Local Hybrid checkpoints
â”‚   â”œâ”€â”€ best_local_hybrid.pt   # Best model (97.52%)
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ training_history.png
â”‚   â””â”€â”€ test_results.json
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ COMPREHENSIVE_MODEL_COMPARISON.md # Full 5-model comparison
â”‚   â”œâ”€â”€ HYPERPARAMETER_TUNING.md # Baseline tuning results
â”‚   â”œâ”€â”€ DATA_SETUP.md          # Data preparation guide
â”‚   â””â”€â”€ PREPROCESSING_PIPELINE.md # Preprocessing details
â”œâ”€â”€ run_train_all.py            # Unified training script (all models)
â”œâ”€â”€ create_comparison_plots.py  # Generate comparison visualizations
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/hyunku9566/Hybrid-Attention-Based_Temporal-Modeling.git
cd baseline-adl-recognition

# Create virtual environment
conda create -n adl python=3.8
conda activate adl

# Install dependencies
pip install -r requirements.txt
```

### Dataset Setup

**Option 1: Use Existing Data (Recommended for reproduction)**

If you have the original ADL dataset:

```bash
# Setup data links (no copying, saves space!)
chmod +x setup_data.sh
./setup_data.sh
```

This links to your existing data:
- ğŸ“ `adl_noerror/` - 120 normal activity files
- ğŸ“ `adl_error/` - 90 error activity files  
- ğŸ“ `processed_all/` - 440 preprocessed binary matrices
- ğŸ“Š `dataset_with_lengths_v3.npz` - Ready-to-use dataset (7,066 sequences)

**Option 2: Prepare Your Own Data**

```bash
# Build features from your preprocessed CSVs
python data/build_features.py \
    --data_dir data/raw/your_data \
    --output data/processed/dataset.npz \
    --T 100 --stride 5
```

**See [docs/DATA_SETUP.md](docs/DATA_SETUP.md) for detailed instructions.**

### Training

```bash
# Train with optimal hyperparameters (95.40% accuracy)
python train/train.py \
    --data_path data/processed/dataset_with_lengths_v3.npz \
    --epochs 50 --batch_size 32 --lr 3e-4 \
    --dropout 0.1 --hidden_dim 256 --focal_gamma 1.5 --patience 15

# Or train with your custom dataset
python train/train.py \
    --data_path data/processed/dataset.npz \
    --epochs 50 --batch_size 32 --lr 3e-4 \
    --dropout 0.1 --hidden_dim 256 --focal_gamma 1.5 --patience 15
```

### Evaluation

```bash
# Evaluate on test set
python evaluate/evaluate.py \
    --model_path checkpoints/best_baseline.pt \
    --data_path processed_data/dataset.npz \
    --output_dir results
```

### Inference

```python
import torch
from models.baseline_model import BaselineModel

# Load model with optimal configuration
model = BaselineModel(in_dim=114, hidden=256, classes=5, dropout=0.1)
model.load_state_dict(torch.load('checkpoints/best_baseline.pt', weights_only=False))
model.eval()

# Predict
with torch.no_grad():
    outputs = model(input_sequence)  # [batch, seq_len, features]
    predictions = outputs.argmax(dim=1)
```

## ğŸ“Š Detailed Results

### Per-Class Performance

| Activity | Precision | Recall | F1-Score | Avg Length |
|----------|-----------|--------|----------|------------|
| Cooking (t1) | 0.882 | 0.918 | 0.896 | 24.5 steps |
| **Hand washing (t2)** | **0.982** | **0.932** | **0.956** â­ | **10.8 steps** |
| Sleeping (t3) | 0.988 | 0.982 | 0.985 | 100.0 steps |
| Taking medicine (t4) | 0.918 | 0.885 | 0.901 | 18.3 steps |
| Eating (t5) | 0.893 | 0.874 | 0.883 | 21.7 steps |

### Why Our Model Excels

**Problem**: Existing HAR models fail on short activities (<15 timesteps)
- SOTA models: t2 F1 = 0.84-0.90
- **Our model: t2 F1 = 0.956** (+5.6-11.6%p improvement)

**Solution**: Data-driven architectural design
- Small receptive field (13 steps) matches median activity duration
- Single attention focus preserves fine-grained patterns
- BiGRU efficiency for short sequences

## ğŸ§ª Ablation Studies

We systematically tested 6 architectural variants:

| Variant | Accuracy | Macro F1 | t2 F1 | Result |
|---------|----------|----------|-------|--------|
| **Baseline** | **93.7%** | **0.924** | **0.956** | âœ… **Optimal** |
| TCN-5 (deeper) | 90.9% | 0.856 | 0.821 | âŒ Over-smoothing |
| Multi-Head 4h | 92.8% | 0.895 | 0.848 | âŒ Feature dilution |
| Multi-Head 8h | 93.0% | 0.889 | 0.857 | âŒ Worse dilution |
| SSL+Graph | 94.5% | 0.907 | 0.822 | âŒ Short seq damage |
| Graph-only | 89.7% | 0.845 | 0.814 | âŒ Implementation issues |
| Adaptive SSL | 93.4% | 0.920 | 0.937 | ğŸŸ¡ Alternative |

**Key Finding**: *"Simplicity beats complexity for ADL recognition"*

## ğŸ“– Hyperparameters

### ğŸ¯ Optimal Configuration (95.40% Accuracy)

```python
# Model Architecture
in_dim = 114           # Sensor features
hidden = 256           # TCN/BiGRU hidden size (increased from 128)
tcn_blocks = 3         # Number of TCN blocks
dilations = [1, 2, 4]  # Exponential dilation
kernel_size = 3        # TCN kernel size
dropout = 0.1          # TCN dropout (reduced from 0.2)

# Training (Optimized)
batch_size = 32        # Reduced for more frequent updates
learning_rate = 3e-4   # Initial learning rate
weight_decay = 1e-4    # L2 regularization
epochs = 50            # Increased training duration
patience = 15          # Early stopping patience (increased)
loss = FocalLoss(gamma=1.5)  # Reduced focusing (from 2.0)
optimizer = AdamW      # With CosineAnnealingWarmRestarts
```

### ğŸ“š Detailed Tuning Results
See **[docs/HYPERPARAMETER_TUNING.md](docs/HYPERPARAMETER_TUNING.md)** for comprehensive experimentation details, performance comparisons, and tuning insights.

## ğŸ“ˆ Visualization

### Attention Weights

Our model learns interpretable temporal patterns:

```bash
python evaluate/visualize.py \
    --model_path checkpoints/best_baseline.pt \
    --data_path processed_data/dataset.npz \
    --output_dir visualizations
```

Example: Hand washing activity shows high attention on water sensor activation/deactivation.
# Hybrid-Attention-Based_Temporal-Modeling
# Hybrid-Attention-Based_Temporal-Modeling
